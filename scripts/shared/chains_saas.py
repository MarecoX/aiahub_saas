from langchain_core.tools import StructuredTool
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.agents.middleware import before_model
from langgraph.checkpoint.postgres import PostgresSaver
from langchain_core.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from google import genai
from openai import BadRequestError
import os
import psycopg
import logging
import asyncio
import sys

# Garante acesso ao saas_db (mesmo diret√≥rio)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# Tenta importar clear_chat_history, fail-safe se saas_db falhar
try:
    from saas_db import clear_chat_history
except ImportError:

    def clear_chat_history(id):
        logging.error("Fun√ß√£o clear_chat_history n√£o encontrada!")
        return False


logger = logging.getLogger("KestraChainsSaaS")

# Configura√ß√£o Global
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    try:
        import streamlit as st

        OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")
    except Exception:
        pass

DATABASE_URL = os.environ.get("DATABASE_CONNECTION_URI") or os.getenv("DATABASE_URL")


# --- MULTIMODIAL HELPERS ---
def transcribe_audio(audio_bytes: bytes) -> str:
    """Transcreve √°udio usando OpenAI Whisper."""
    try:
        from openai import OpenAI
        import io

        client = OpenAI(api_key=OPENAI_API_KEY)

        # Cria um arquivo em mem√≥ria com nome fake .mp3 para o Whisper aceitar
        audio_file = io.BytesIO(audio_bytes)
        audio_file.name = "audio.mp3"

        transcript = client.audio.transcriptions.create(
            model="whisper-1", file=audio_file, language="pt"
        )
        return transcript.text
    except Exception as e:
        logger.error(f"Erro na transcri√ß√£o de √°udio: {e}")
        return f"[Erro ao transcrever √°udio: {e}]"


# --- DATABASE / CHECKPOINTER SETUP ---
# Conex√£o lazy para evitar conex√µes stale que o PostgreSQL fecha
_checkpointer = None
_conn = None


def get_checkpointer():
    """Retorna checkpointer, reconectando se necess√°rio."""
    global _checkpointer, _conn

    if not DATABASE_URL:
        logger.warning("DATABASE_URL n√£o encontrada. Checkpointer desabilitado.")
        return None

    try:
        # Testa se a conex√£o ainda est√° viva
        if _conn is not None:
            try:
                _conn.execute("SELECT 1")
            except Exception:
                logger.warning("‚ö†Ô∏è Conex√£o PostgreSQL stale detectada. Reconectando...")
                _conn = None
                _checkpointer = None

        # Cria nova conex√£o se necess√°rio
        if _conn is None:
            _conn = psycopg.connect(DATABASE_URL, autocommit=True)
            _checkpointer = PostgresSaver(conn=_conn)
            _checkpointer.setup()
            logger.info("‚úÖ PostgresSaver Checkpointer (re)conectado com sucesso.")

        return _checkpointer
    except Exception as e:
        logger.error(f"‚ùå Falha ao configurar Checkpointer: {e}")
        return None


# --- TOOLS ---


# Configura Gemini Client
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_KEY:
    try:
        import streamlit as st

        GEMINI_KEY = st.secrets.get("GEMINI_API_KEY")
    except Exception:
        pass
gemini_client = None
if GEMINI_KEY:
    try:
        gemini_client = genai.Client(api_key=GEMINI_KEY)
    except Exception as e:
        logger.error(f"Erro init Gemini Client: {e}")

# --- DYNAMIC TOOL FACTORY ---

# Acumulador de uso do Gemini RAG (resetado a cada ask_saas)
_gemini_usage_accumulator = {"input_tokens": 0, "output_tokens": 0}


def create_knowledge_base_tool(store_id: str):
    """
    Cria uma ferramenta de busca din√¢mica ligada a um Vector Store (Enterprise) espec√≠fico.
    """

    # Cache simples para evitar chamadas repetidas com mesma query
    _rag_cache = {}

    def search_func(query: str):
        if not gemini_client:
            return "Erro: Client Gemini n√£o configurado."

        # Verifica cache (evita loop infinito)
        cache_key = f"{store_id}:{query}"
        if cache_key in _rag_cache:
            logger.info(f"üìö RAG Cache Hit: {query[:50]}...")
            return _rag_cache[cache_key]

        max_retries = 2
        original_query = query

        for attempt in range(max_retries + 1):
            try:
                logger.info(
                    f"üìö RAG Enterprise (v2-FIX): {store_id} | Query: {query} | Attempt: {attempt + 1}"
                )

                # Padr√£o Enterprise: Queries usando a tool File Search no generate_content
                # Adiciona instru√ß√£o FORTE de idioma para garantir resposta em portugu√™s
                prompt_with_lang = (
                    f"IMPORTANTE: Responda APENAS em portugu√™s brasileiro (pt-BR). "
                    f"N√ÉO responda em espanhol ou ingl√™s. "
                    f"N√ÉO pe√ßa mais informa√ß√µes, busque diretamente nos documentos. "
                    f"Busque nos documentos e responda em portugu√™s: {query}"
                )

                response = gemini_client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt_with_lang,
                    config={
                        "tools": [
                            {"file_search": {"file_search_store_names": [store_id]}}
                        ]
                    },
                )

                # Retorna o texto gerado (que √© a resposta baseada nos docs)
                if response.text:
                    # Limita resposta a 2000 chars para evitar overflow no OpenAI
                    result = response.text[:2000]

                    # DETEC√á√ÉO DE CLARIFICA√á√ÉO: Se Gemini pediu mais detalhes ao inv√©s de buscar
                    clarification_indicators = [
                        "podr√≠a",
                        "podr√≠as",
                        "¬ø",
                        "refiere",
                        "especificar",
                        "poderia",
                        "voc√™ quis dizer",
                        "qual tipo",
                        "que tipo",
                        "mais detalhes",
                        "esclarecer",
                        "could you",
                    ]
                    is_clarification = "?" in result[:200] and any(
                        indicator in result.lower()[:300]
                        for indicator in clarification_indicators
                    )

                    if is_clarification and attempt < max_retries:
                        logger.warning(
                            f"‚ö†Ô∏è RAG pediu clarifica√ß√£o, retry {attempt + 1}/{max_retries}"
                        )
                        # Retry com query mais direta
                        query = f"Liste TODAS as informa√ß√µes dispon√≠veis sobre: {original_query}"
                        continue

                    # Sucesso - cachear e retornar
                    _rag_cache[cache_key] = result

                    # DEBUG: Log o que o Gemini retornou
                    logger.info(
                        f"üìö RAG Response (primeiros 300 chars): {result[:300]}..."
                    )

                    # Acumula usage do Gemini para tracking
                    global _gemini_usage_accumulator
                    if hasattr(response, "usage_metadata") and response.usage_metadata:
                        _gemini_usage_accumulator["input_tokens"] += getattr(
                            response.usage_metadata, "prompt_token_count", 0
                        )
                        _gemini_usage_accumulator["output_tokens"] += getattr(
                            response.usage_metadata, "candidates_token_count", 0
                        )

                    return result

                logger.warning(f"‚ö†Ô∏è RAG retornou resposta vazia para query: {query}")
                return "Sem informa√ß√µes relevantes encontradas nos documentos."

            except Exception as e:
                logger.error(f"Erro RAG Enterprise: {e}", exc_info=True)
                if attempt < max_retries:
                    continue
                return f"Erro ao consultar Base de Conhecimento: {str(e)}"

        return "Sem informa√ß√µes relevantes encontradas nos documentos."

    return StructuredTool.from_function(
        func=search_func,
        name="consultar_documentos_empresa",
        description="Use esta ferramenta para buscar informa√ß√µes nos manuais, PDFs e arquivos da empresa. O Gemini pesquisar√° internamente e retornar√° a resposta baseada nos documentos em portugu√™s",
    )


# --- FACTORY ---


def create_saas_agent(system_prompt: str, tools_list: list, store_id: str = None):
    """
    Cria um Agente OpenAI usando create_agent e PostgresSaver.
    Injeta dinamicamente o tool de Knowledge Base se store_id for v√°lido.
    """
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5, api_key=OPENAI_API_KEY)

    final_tools = list(tools_list) if tools_list else []

    # Injeta Knowledge Base do Cliente (Enterprise usa nomes resource, ex: projects/... ou stores/...)
    # Nosso store_id local pode ser o 'name' completo ou apenas o ID.
    # O Gemini Manager v2 tenta usar o name completo.
    # Assume-se que store_id venha correto do DB (atualizado pelo Manager).

    if store_id:
        kb_tool = create_knowledge_base_tool(store_id)
        final_tools.append(kb_tool)
        logger.info(f"üìé Tool Enterprise Docs injetada: {store_id}")

    # --- CONTEXT TRIMMING (LangChain 1.0 Strict) ---

    # Middleware para Trimming (Max 20 mensagens)
    @before_model
    def trim_middleware(state, runtime) -> dict | None:
        messages = state["messages"]
        # Mant√©m System + √öltimas 20 (aprox. 8k tokens)
        if len(messages) <= 20:
            return None

        # Limpa tudo e reinserir as ultimas 20
        return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES), *messages[-20:]]}

    logger.info("‚úÇÔ∏è Context Trimmer ativado (Middleware LangChain 1.0).")

    return create_agent(
        model=llm,
        tools=final_tools,
        system_prompt=system_prompt,
        checkpointer=get_checkpointer(),
        middleware=[trim_middleware],
        # Limita itera√ß√µes para evitar loops infinitos de tool calls
        recursion_limit=15,
    )


# --- INTERFACE ---


async def ask_saas(
    query: str,
    chat_id: str,
    system_prompt: str,
    client_config: dict,
    tools_list: list = None,
    image_base64: str = None,
    audio_bytes: bytes = None,
):
    global _conn, _checkpointer  # Para poder resetar a conex√£o

    # 1. Processa √Åudio (Se houver)
    if audio_bytes:
        transcription = await asyncio.to_thread(transcribe_audio, audio_bytes)
        # Se query veio vazia (s√≥ audio), usa a transcri√ß√£o
        if not query:
            query = transcription
        else:
            query = f"{query}\n[Transcri√ß√£o de √Åudio]: {transcription}"

    # 2. Constr√≥i Mensagem do Usu√°rio (Multimodal se houver imagem)
    from langchain_core.messages import HumanMessage

    if image_base64:
        # GPT-4o aceita lista de conteudos
        user_content = [
            {"type": "text", "text": query},
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"},
            },
        ]
        user_message = HumanMessage(content=user_content)
    else:
        # Texto simples
        user_message = ("user", query)

    tools = tools_list or []
    store_id = client_config.get("gemini_store_id")

    # Reseta acumulador de Gemini usage
    global _gemini_usage_accumulator
    _gemini_usage_accumulator = {"input_tokens": 0, "output_tokens": 0}

    # Retry loop para lidar com conex√µes stale
    max_retries = 2
    for attempt in range(max_retries):
        try:
            # 1. Cria o Agente (Passando Store ID)
            agent_runnable = create_saas_agent(system_prompt, tools, store_id=store_id)

            # 2. Config de Execu√ß√£o (thread_id inclui client_id para isolar contextos)
            client_id = str(client_config.get("id", "unknown"))
            thread_id = (
                f"{client_id}:{chat_id}"  # Cada cliente SaaS tem hist√≥rico separado
            )
            config = {"configurable": {"thread_id": thread_id}}

            # 3. Executa com Prote√ß√£o
            try:
                result = await asyncio.to_thread(
                    agent_runnable.invoke,
                    {"messages": [user_message]},
                    config=config,
                )
            except BadRequestError as e:
                # AUTO-HEALING: Detecta erro de tool_calls pendentes e limpa
                error_str = str(e)
                if "tool_calls" in error_str or "400" in error_str:
                    logger.warning(
                        f"üö® Hist√≥rico corrompido detectado para {thread_id}. Iniciando Auto-Limpeza..."
                    )
                    await asyncio.to_thread(
                        clear_chat_history, thread_id
                    )  # Usa thread_id composto
                    return (
                        "‚ö†Ô∏è [Auto-Corre√ß√£o] Detectei um erro na minha mem√≥ria recente. Reiniciei nosso contexto. Por favor, fa√ßa sua pergunta novamente.",
                        {"openai": None, "gemini": None},
                    )
                raise e

            # 4. Processa Resposta
            messages = result.get("messages", [])

            # Captura usage para tracking
            usage_data = {"openai": None, "gemini": _gemini_usage_accumulator.copy()}
            # Tenta extrair usage do OpenAI (via response_metadata)
            if messages and hasattr(messages[-1], "response_metadata"):
                token_usage = messages[-1].response_metadata.get("token_usage", {})
                if token_usage:
                    usage_data["openai"] = {
                        "input_tokens": token_usage.get("prompt_tokens", 0),
                        "output_tokens": token_usage.get("completion_tokens", 0),
                    }

            if messages:
                return messages[-1].content, usage_data
            else:
                return "Erro: Nenhuma resposta gerada.", usage_data

        except psycopg.OperationalError as e:
            # CONEX√ÉO STALE - Reconecta e tenta novamente
            logger.warning(
                f"‚ö†Ô∏è Conex√£o PostgreSQL perdida (tentativa {attempt + 1}/{max_retries}): {e}"
            )
            _conn = None
            _checkpointer = None

            if attempt < max_retries - 1:
                logger.info("üîÑ Reconectando e tentando novamente...")
                continue
            else:
                logger.error("‚ùå Falha ap√≥s todas as tentativas de reconex√£o")
                return (
                    "Desculpe, tive um problema de conex√£o. Por favor, tente novamente.",
                    {"openai": None, "gemini": None},
                )

        except Exception as e:
            logger.error(f"Erro no Agent SaaS: {e}", exc_info=True)
            return "Desculpe, tive um erro interno ao processar sua solicita√ß√£o.", {
                "openai": None,
                "gemini": None,
            }
