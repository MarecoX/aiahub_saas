# Arquitetura do Sistema ğŸ›ï¸

## Modular Monolith
O Kestra 2.0 segue o padrÃ£o **Modular Monolith**. Isso significa que todos os serviÃ§os rodam no mesmo repositÃ³rio e processo, mas sÃ£o logicamente separados.

## ğŸ—ºï¸ Mapa do Codebase

## ğŸ—ºï¸ Mapa Detalhado dos MÃ³dulos

### 1. `/api` (Backend)
Gateway de entrada. Feito em FastAPI.
*   `routers/meta.py`: Recebe os Webhooks da Meta (WhatsApp Oficial). Valida assinatura (`hub.verify_token`) e despacha para fila/processamento.
*   `routers/clients.py`: CRUD de usuÃ¡rios e configuraÃ§Ãµes do SaaS.
*   `services/meta_service.py`: Camada de serviÃ§o que processa o payload bruto do WhatsApp antes de salvar.

### 2. `/scripts` (Core Logic)
O cÃ©rebro do sistema. Scripts executados pelos workers do Kestra.

#### ğŸŸ¢ MÃ³dulo: `meta` (WhatsApp Oficial)
*   **`meta_manager.py`:** Orquestrador principal. Recebe mensagem -> Identifica Cliente -> Carrega HistÃ³rico -> Chama LangChain -> Envia Resposta.
*   **`meta_client.py`:** Wrapper HTTP oficial. MÃ©todos para `send_text`, `send_image`, `upload_media`, `mark_read`.
*   **`meta_oauth.py`:** Gerencia o fluxo de "Embedded Signup" (Login com Facebook).

#### ğŸŸ¡ MÃ³dulo: `shared` (Bibliotecas Comuns)
*   **`saas_db.py`:** **[CRÃTICO]** Gerencia o pool de conexÃµes (psycopg_pool). ContÃ©m todas as queries SQL (buscas de cliente, salvamento de mensagem).
*   **`media_utils.py`:** Processamento Multimodal (Exclusivo Meta Oficial).
    *   **Nota:** Processa Ã¡udio (Whisper) e imagens (Gemini) vindos da API Cloud. NÃ£o Ã© utilizado pelo Uazapi ou LancePilot.
    *   `transcribe_audio_bytes()`: Transcreve Ã¡udio.
    *   `analyze_image_bytes()`: Descreve imagens.
*   **`tools_library.py`:** DefiniÃ§Ã£o das Tools (Calendar, CRM) que a IA pode usar.

#### ğŸŸ£ MÃ³dulo: `lancepilot` (Legacy/Mass)
*   `ingest.py`: Recebe webhook e faz "Debounce" (espera usuÃ¡rio parar de digitar).
*   `rag_worker.py`: Pipeline RAG. Busca documentos no Google Gemini Vector Store e gera resposta.
*   `sender.py`: Dispara a resposta final via API do LancePilot.

---

## ğŸ”„ Fluxos de Dados (Arquitetura HÃ­brida)

O sistema opera em dois modos distintos: **Tempo Real (API)** e **Orquestrado (Kestra)**.

### Fluxo A: API Oficial (Meta Cloud) âš¡
**ExecuÃ§Ã£o:** FastAPI (Background Tasks)
**NÃ£o passa pelo Kestra.** A Meta exige respostas em <3s, entÃ£o processamos tudo na memÃ³ria da API.

1.  **Webhook:** Meta chama `POST /meta/webhook`.
2.  **FastAPI:** Valida a assinatura de seguranÃ§a.
3.  **Processamento (`meta_manager.py`):**
    *   Identifica o Tenant pelo `display_phone_number`.
    *   Salva a mensagem no Postgres (`chat_messages`).
    *   **IA Decision:** Se o bot estiver ativo, chama o `langchain` para gerar resposta.
4.  **Envio:** `meta_client.py` dispara a resposta HTTP de volta para a Meta.

### Fluxo B: LancePilot e Uazapi âš™ï¸
**ExecuÃ§Ã£o:** Kestra Workflow Engine
Processos assÃ­ncronos, em massa ou agendados.

#### 1. LancePilot (Disparo em Massa)
Definido em: `flows/lancepilot_native.yaml`
Este fluxo Ã© um **Pipeline** linear:
*   **Trigger:** Webhook do LancePilot (Cliente respondeu campanha).
*   **Step 1 (Ingest):** `ingest.py` recebe o JSON, valida e salva num buffer Redis (evita processar cada letra digitada).
*   **Step 2 (RAG):** `rag_worker.py` lÃª o buffer, busca contexto na Base de Conhecimento (Google Gemini) e gera a resposta via GPT/Gemini.
*   **Step 3 (Sender):** `sender.py` envia a resposta final para a API do LancePilot.

#### 2. Uazapi (Follow-up)
Definido em: `flows/saas_followup_cron.yaml`
Este fluxo Ã© um **Loop Agendado** (Cron):
*   **Trigger:** A cada 5 minutos.
*   **Step 1:** O Kestra sobe um container Docker rodando `scripts/uazapi/followup_worker.py`.
*   **LÃ³gica:** O script varre o banco buscando conversas "mornas" (sem resposta hÃ¡ X horas).
*   **AÃ§Ã£o:** Se a IA decidir que vale a pena, envia uma mensagem "E aÃ­, vamos fechar?" usando a API Uazapi.

---

## ğŸ› ï¸ Como Cada MÃ³dulo Funciona (Deep Dive)

### MÃ³dulo `scripts/meta`
Focado em **Alta Performance**.
*   **`meta_client.py`:** Ã‰ a "mÃ£o" do sistema. Sabe montar o JSON exato que a API do WhatsApp exige (Templates, Imagens, BotÃµes).
*   **`meta_manager.py`:** Ã‰ o "cÃ©rebro" rÃ¡pido. Ele decide: "Ã‰ mensagem de texto? Ã‰ Ã¡udio? O cliente estÃ¡ pausado (Human Handover)?".

### MÃ³dulo `scripts/shared`
A "Cola" que une o sistema.
*   **`saas_db.py`:** Ãšnico ponto de contato com o banco. Se mudarmos a tabela `clients`, sÃ³ alteramos aqui.
*   **`media_utils.py`:** Transforma "BinÃ¡rio" em "Texto". Recebe bytes de Ã¡udio OGG do WhatsApp, manda pra OpenAI (Whisper) e devolve string.

### MÃ³dulo `flows/` (YAMLs do Kestra)
SÃ£o as "Receitas de Bolo" que o Kestra segue.
*   Eles definem a **Infraestrutura**: "Use a imagem Docker `marsllator/my-kestra-worker`".
*   Eles definem as **VariÃ¡veis**: "Passe a senha do Banco e a API Key da OpenAI para o script Python".

---

## ğŸ›ï¸ PadrÃµes de Design

PostgreSQL Ã© a fonte da verdade.

*   `clients`: Tabela mestre. Cada linha Ã© um SaaS Tenant.
*   `active_conversations`: Estado atual da conversa (State Machine).
*   `chat_messages`: HistÃ³rico de mensagens (Log).

> **DecisÃ£o de Design (ADR-001):** ConfiguraÃ§Ãµes de ferramentas (ex: Calendar, CEP) sÃ£o salvas em uma coluna `JSONB` chamada `tools_config` dentro da tabela `clients`. Isso permite flexibilidade sem migrations constantes.

> **DecisÃ£o de Design (ADR-002 - 2026-01):** ConfiguraÃ§Ãµes de **provedores de comunicaÃ§Ã£o** (Uazapi, LancePilot, Meta) foram movidas para a tabela `client_providers`. Isso permite mÃºltiplas instÃ¢ncias do mesmo provedor por cliente e separaÃ§Ã£o clara de responsabilidades. Os workers usam sistema de fallback para retrocompatibilidade.

> **DecisÃ£o de Design (ADR-003 - 2026-02):** MÃ©tricas da IA usam arquitetura de **3 camadas**: Event Log (`conversation_events`) â†’ Worker de AgregaÃ§Ã£o (cron 5min) â†’ Tabela PrÃ©-calculada (`metrics_daily`). Isso garante leitura instantÃ¢nea no dashboard (<50ms) independente do volume de dados, sem impactar a performance dos workers de ingest/RAG. Ver detalhes completos em `docs/database.md`.

## ğŸ”„ Compatibilidade e MigraÃ§Ã£o (Fallback Strategy)

Para garantir que clientes antigos continuem funcionando enquanto migramos para `client_providers`, o sistema implementa a seguinte lÃ³gica de prioridade na resoluÃ§Ã£o de credenciais (ex: em `rag_worker.py`):

### Ordem de ResoluÃ§Ã£o (Priority List):
1.  **Tabela `client_providers` (New):** O sistema busca primeiro por um registro ativo para o `provider_type` correspondente (uazapi, lancepilot, meta).
2.  **ConfiguraÃ§Ã£o Legada (`clients` table):** Se nÃ£o encontrar no provider, busca nas colunas antigas:
    *   **API URL:** `clients.api_url` ou `clients.tools_config['whatsapp']['url']`
    *   **Token/Key:** `clients.token` ou `clients.tools_config['whatsapp']['key']`

> **Nota:** O objetivo Ã© depreciar as colunas `token`, `api_url` e `whatsapp_provider` da tabela `clients` apÃ³s a migraÃ§Ã£o completa de todos os tenants.

---

## ğŸ“Š Pipeline de MÃ©tricas (ADR-003)

O painel de mÃ©tricas da IA opera com 3 camadas para performance em escala:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ingest / RAG  â”‚â”€â”€â”€â”€â–¶â”‚  conversation_events  â”‚â”€â”€â”€â”€â–¶â”‚  metrics_daily  â”‚
â”‚   Workers       â”‚     â”‚  (append-only log)    â”‚     â”‚  (prÃ©-agregado) â”‚
â”‚                 â”‚     â”‚  ~0ms por INSERT      â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                              â”‚
                               â”‚  metrics_worker (cron 5min)  â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                              â”‚
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚    Dashboard      â”‚
                                                    â”‚  (leitura <50ms)  â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Por que esta arquitetura?

| Alternativa | Escrita | Leitura Dashboard | Escala |
| :--- | :--- | :--- | :--- |
| Query ao vivo (full scan) | Nenhuma | Lenta | Ruim |
| SÃ³ event log | 1 INSERT | MÃ©dia (agregaÃ§Ã£o) | Boa |
| **Event log + agregaÃ§Ã£o (escolhido)** | **1 INSERT** | **InstantÃ¢nea** | **Excelente** |
| Time-series DB externo | RÃ¡pida | RÃ¡pida | Excelente (+ infra) |

A abordagem escolhida usa somente PostgreSQL (sem nova infra) e permite derivar qualquer mÃ©trica futura a partir do event log imutÃ¡vel.
